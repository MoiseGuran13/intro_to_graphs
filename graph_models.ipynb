{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch_geometric\n",
    "from utils import vis_grid, vis_from_pyg\n",
    "from torch_geometric.datasets import MoleculeNet\n",
    "from torch_geometric.loader import DataLoader\n",
    "import random\n",
    "\n",
    "\n",
    "dataset = MoleculeNet(\"datasets/\", \"BACE\")\n",
    "atom_dims = dataset.num_node_features\n",
    "bond_dims = dataset.num_edge_features\n",
    "\n",
    "# Convert dataset to a list of Data objects\n",
    "data_list = list(dataset)\n",
    "# Shuffle (these datasets have all the labels at one end)\n",
    "random.shuffle(data_list)\n",
    "dataset = data_list\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to make it a dataloader, instead of a dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_prop, val_prop, test_prop = 0.6, 0.2, 0.2\n",
    "n_train, n_val, n_test = int(train_prop*len(dataset)), int(val_prop*len(dataset)), int(test_prop*len(dataset))\n",
    "train, val, test = dataset[:n_train], dataset[n_train:-n_test], dataset[-n_test:]\n",
    "train_loader = DataLoader(train, batch_size = 64, shuffle = True)\n",
    "val_loader = DataLoader(val, batch_size = 64, shuffle = True)\n",
    "test_loader = DataLoader(test, batch_size = 64, shuffle = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining a model\n",
    "\n",
    "A graph model has a few parts:\n",
    "- Projection heads: Learn to map node and edge features to a new space (often linear layers)\n",
    "- GNN layers: Graph layers. Here we'll use message passing layers, but graph transformers are also an option.\n",
    "- Pooling: Aggregate the node embeddings of the GNN layers into one vector.\n",
    "- Output: Standard output stuff, vector to output space.\n",
    "\n",
    "We'll work through part-by-part, then bring them together to get a model.\n",
    "\n",
    "First we'll do the node/edge feature encoders. These are just a few linear layers, where we specify input and output size.\n",
    "\n",
    "*We'll also use this as an output layer*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn import Sequential, Linear, ReLU\n",
    "import numpy as np\n",
    "\n",
    "class GenericEncoder(torch.nn.Module):\n",
    "\t\"\"\"\n",
    "\tA generic encoder module that transforms input features into embeddings.\n",
    "\n",
    "\tArgs:\n",
    "\t\temb_dim (int): The dimensionality of the output embeddings.\n",
    "\t\tfeat_dim (int): The dimensionality of the input features.\n",
    "\t\tn_layers (int, optional): The number of layers in the encoder. Defaults to 1.\n",
    "\t\"\"\"\n",
    "\n",
    "\tdef __init__(self, emb_dim, feat_dim, n_layers=1):\n",
    "\t\tsuper(GenericEncoder, self).__init__()\n",
    "\t\tself.layers = []\n",
    "\t\tspread_layers = [min(emb_dim, feat_dim) + np.abs(feat_dim - emb_dim) * i for i in range(n_layers - 1)]\n",
    "\n",
    "\t\tlayer_sizes = [feat_dim] + spread_layers + [emb_dim]\n",
    "\t\tfor i in range(n_layers):\n",
    "\t\t\tlin = Linear(layer_sizes[i], layer_sizes[i + 1])\n",
    "\t\t\ttorch.nn.init.xavier_uniform_(lin.weight.data)\n",
    "\t\t\tself.layers.append(lin)\n",
    "\n",
    "\t\t\tif i != n_layers:\n",
    "\t\t\t\tself.layers.append(ReLU())\n",
    "\n",
    "\t\tself.model = Sequential(*self.layers)\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\t\"\"\"\n",
    "\t\tForward pass of the encoder.\n",
    "\n",
    "\t\tArgs:\n",
    "\t\t\tx (torch.Tensor): Input features.\n",
    "\n",
    "\t\tReturns:\n",
    "\t\t\ttorch.Tensor: Output embeddings.\n",
    "\t\t\"\"\"\n",
    "\t\treturn self.model(x.float())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the GNN part:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv\n",
    "from torch_geometric.nn import global_add_pool\n",
    "\n",
    "# Generic N-Layer Graph Isomorphism Network encoder\n",
    "class GNNBlock(torch.nn.Module):\n",
    "    def __init__(self, emb_dim=300, num_gc_layers=5, drop_ratio=0.0):\n",
    "        super(GNNBlock, self).__init__()\n",
    "\n",
    "        self.emb_dim = emb_dim\n",
    "        self.num_gc_layers = num_gc_layers\n",
    "        self.drop_ratio = drop_ratio\n",
    "        self.out_node_dim = self.emb_dim\n",
    "\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "        self.bns = torch.nn.ModuleList()\n",
    "\n",
    "        self.convolution = GINEConv\n",
    "\n",
    "        for i in range(num_gc_layers):\n",
    "            nn = Sequential(Linear(emb_dim, 2 * emb_dim), torch.nn.BatchNorm1d(2 * emb_dim), ReLU(),\n",
    "                            Linear(2 * emb_dim, emb_dim))\n",
    "            conv = GINEConv(nn)\n",
    "            bn = torch.nn.BatchNorm1d(emb_dim)\n",
    "            self.convs.append(conv)\n",
    "            self.bns.append(bn)\n",
    "\n",
    "    def init_emb(self):\n",
    "        \"\"\"\n",
    "        Initializes the node embeddings.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, batch, x, edge_index, edge_attr):\n",
    "        # compute node embeddings using GNN\n",
    "        xs = []\n",
    "        for i in range(self.num_gc_layers):\n",
    "            x = self.convs[i](x, edge_index, edge_attr)\n",
    "            x = self.bns[i](x)\n",
    "\n",
    "            if i == self.num_gc_layers - 1:\n",
    "                # remove relu for the last layer\n",
    "                x = F.dropout(x, self.drop_ratio, training=self.training)\n",
    "            else:\n",
    "                x = F.dropout(F.relu(x), self.drop_ratio, training=self.training)\n",
    "            xs.append(x)\n",
    "        return x, xs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now put it all together!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GINEConv\n",
    "\n",
    "# Generic N-Layer Graph Isomorphism Network encoder\n",
    "class GraphModel(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 node_encoder,\n",
    "                 edge_encoder,\n",
    "                 gnn_block,\n",
    "                 output_layer,\n",
    "                 pooling_type = \"standard\"):\n",
    "        super(GraphModel, self).__init__()\n",
    "\n",
    "        self.node_encoder = node_encoder\n",
    "        self.edge_encoder = edge_encoder\n",
    "        self.gnn_block = gnn_block\n",
    "        self.output_layer = output_layer\n",
    "        self.pooling_type = pooling_type\n",
    "\n",
    "        self.init_emb()\n",
    "\n",
    "    def init_emb(self):\n",
    "        \"\"\"\n",
    "        Initializes the node embeddings.\n",
    "        \"\"\"\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, Linear):\n",
    "                torch.nn.init.xavier_uniform_(m.weight.data)\n",
    "                if m.bias is not None:\n",
    "                    m.bias.data.fill_(0.0)\n",
    "\n",
    "    def forward(self, data):\n",
    "        # print(data.x, data.edge_attr)\n",
    "        x = data.x\n",
    "        edge_attr = data.edge_attr\n",
    "        edge_index = data.edge_index\n",
    "        batch = data.batch\n",
    "\n",
    "        x = self.node_encoder.forward(x)\n",
    "        edge_attr = self.edge_encoder.forward(edge_attr)\n",
    "\n",
    "\n",
    "        x, xs = self.gnn_block.forward(batch, x, edge_index, edge_attr)\n",
    "\n",
    "        # compute graph embedding using pooling\n",
    "        if self.pooling_type == \"standard\":\n",
    "            xpool = global_add_pool(x, batch)\n",
    "            \n",
    "\n",
    "        elif self.pooling_type == \"layerwise\":\n",
    "            xpool = [global_add_pool(x, batch) for x in xs]\n",
    "            xpool = torch.cat(xpool, 1)\n",
    "            \n",
    "\n",
    "        x = self.output_layer.forward(xpool)\n",
    "\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_dim   = 1 # Binary classification\n",
    "\n",
    "emb_dim = 64\n",
    "n_layers = 3\n",
    "\n",
    "atom_encoder = GenericEncoder(emb_dim, atom_dims)\n",
    "bond_encoder = GenericEncoder(emb_dim, bond_dims)\n",
    "out_layer    = GenericEncoder(out_dim, emb_dim)\n",
    "gnn_block = GNNBlock(emb_dim, n_layers, 0.1)\n",
    "\n",
    "model = GraphModel(atom_encoder,\n",
    "                   bond_encoder,\n",
    "                   gnn_block,\n",
    "                   out_layer)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "# Define the training function\n",
    "def train_model(model, train_loader, val_loader, num_epochs=50, learning_rate=0.001):\n",
    "    # Set the model to training mode\n",
    "    model.train()\n",
    "    \n",
    "    # Define the loss function and optimizer\n",
    "    criterion = torch.nn.BCEWithLogitsLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        running_loss = 0.0\n",
    "        for data in train_loader:\n",
    "            # Zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            # Forward pass\n",
    "            outputs = model(data)\n",
    "            loss = criterion(outputs, data.y.float().view(-1, 1))\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Print statistics\n",
    "            running_loss += loss.item()\n",
    "        \n",
    "        # Print epoch loss\n",
    "        # print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {running_loss/len(train_loader)}\")\n",
    "        if epoch % 10 == 0:\n",
    "        # Validate the model\n",
    "            validate_model(model, val_loader)\n",
    "        \n",
    "    print('Finished Training')\n",
    "\n",
    "# Define the validation function\n",
    "def validate_model(model, val_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        preds = []\n",
    "        trues = []\n",
    "        for data in val_loader:\n",
    "            outputs = model(data)\n",
    "            predicted = (torch.sigmoid(outputs) > 0.5).float()\n",
    "            preds += predicted.numpy().tolist()\n",
    "            trues += data.y.numpy().tolist()\n",
    "    \n",
    "    correct = np.sum(np.array(preds) == np.array(trues))\n",
    "    total = len(preds)\n",
    "    print(f'Validation Accuracy: {100 * correct / total:.2f}%')\n",
    "    weighting = np.sum(trues) / len(trues)\n",
    "    print(f\"Dataset weighting: {weighting}\")\n",
    "\n",
    "# Train the model\n",
    "train_model(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise\n",
    "\n",
    "- Visualise the validation and train loss\n",
    "- Write a `test_model` function to run over the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another exercise\n",
    "\n",
    "**ESOL** is a regression dataset, predicting the solubility of various molecules. Adapt the above code for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = MoleculeNet(\"datasets/\", \"ESOL\")\n",
    "atom_dims = dataset.num_node_features\n",
    "bond_dims = dataset.num_edge_features\n",
    "\n",
    "# Convert dataset to a list of Data objects\n",
    "data_list = list(dataset)\n",
    "# Shuffle (these datasets have all the labels at one end)\n",
    "random.shuffle(data_list)\n",
    "dataset = data_list\n",
    "\n",
    "train_prop, val_prop, test_prop = 0.6, 0.2, 0.2\n",
    "n_train, n_val, n_test = int(train_prop*len(dataset)), int(val_prop*len(dataset)), int(test_prop*len(dataset))\n",
    "train, val, test = dataset[:n_train], dataset[n_train:-n_test], dataset[-n_test:]\n",
    "train_loader = DataLoader(train, batch_size = 64, shuffle = True)\n",
    "val_loader = DataLoader(val, batch_size = 64, shuffle = True)\n",
    "test_loader = DataLoader(test, batch_size = 64, shuffle = True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "intro_pyg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
